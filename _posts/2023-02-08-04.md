---
title: Training language models to follow instructions with human feedback 论文阅读和复现
data: 2023-02-05 13:00:00
categories: [学习笔记]
tags: [学习笔记]
math: true
mermaid: true
---
# Training language models to follow instructions with human feedback 论文阅读和复现

## Abstract

Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by **fine-tuning with human feedback**. **Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior,** which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.

使语言模型更大并不能从本质上使它们更好地遵循用户的意图。例如，大型语言模型可能会生成不真实的、有毒的或对用户没有帮助的输出。换句话说，这些模型与其用户不一致。在本文中，我们展示了一种途径，可以通过根据人类反馈进行微调，使语言模型与用户对各种任务的意图保持一致。从一组标记器编写的提示和通过 OpenAI API 提交的提示开始，我们收集了所需模型行为的标记器演示数据集，我们用它来使用监督学习微调 GPT-3。然后，我们收集模型输出排名的数据集，我们使用该数据集通过人类反馈的强化学习进一步微调该监督模型。我们将生成的模型称为**InstructGPT**。在对我们的提示分布的人工评估中，1.3B 参数 InstructGPT 模型的输出优于 175B GPT-3 的输出，尽管参数少 100 倍。此外，InstructGPT 模型显示了真实性的提高和有毒输出生成的减少，同时对公共 NLP 数据集的性能回归最小。尽管 InstructGPT 仍然会犯一些简单的错误，但我们的结果表明，根据人类反馈进行微调是使语言模型与人类意图保持一致的一个有前途的方向。

## 1 Introduction
**Large language models (LMs)** can be “prompted” to perform a range of natural language processing (NLP) tasks, given some examples of the task as input. However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020). This is because the language modeling objective used for many recent large LMs—predicting the next token on a webpage from the internet—is different from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Thus, we say that the language modeling objective is misaligned. Averting these unintended behaviors is especially important for language models that are deployed and used in hundreds of applications.

我们可以“提示”大型语言模型 (LM) 执行一系列自然语言处理 (NLP) 任务，并将一些任务示例作为输入。然而，这些模型经常表现出意想不到的行为，例如编造事实、生成有偏见或有毒的文本，或者根本不遵循用户指令（Bender 等人，2021 年；Bommasani 等人，2021 年；Kenton 等人，2021 年；Weidinger 等人）等人，2021 年；Tamkin 等人，2021 年；Gehman 等人，2020 年）。这是因为用于许多最近的大型LM语言建模的目标（从互联网预测网页上的下一个令牌）不同于“有益且安全地遵循用户的指示”的目标（Radford 等人，2019 年；Brown 等人，2020 年；Fedus 等人., 2021 年；Rae 等人，2021 年；Thoppilan 等人，2022 年）。因此，我们说语言建模目标未对齐。避免这些意外行为对于在数百个应用程序中部署和使用的语言模型尤为重要。

We make progress on **aligning language models** by training them to act in accordance with the user’s intention (Leike et al., 2018). This encompasses both explicit intentions such as following instructions and implicit intentions such as staying truthful, and not being biased, toxic, or otherwise harmful. Using the language of Askell et al. (2021), we want language models to be helpful (they should help the user solve their task), honest (they shouldn’t fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment). We elaborate on the evaluation of these criteria in Section 3.6.

通过训练语言模型按照用户的意图行事，我们在对齐语言模型方面取得了进展（Leike 等人，2018 年）。这既包括明确的意图，例如遵循指示，也包括隐含的意图，例如保持真实、不带偏见、有毒或以其他方式有害。使用 Askell 等人的语言。 (2021)，我们希望语言模型有帮助（它们应该帮助用户解决他们的任务）、诚实（它们不应该捏造信息或误导用户）和无害（它们不应该造成身体、心理或社会伤害对人或环境）。我们在第 3.6 节中详细说明了对这些标准的评估。



We focus on fine-tuning approaches to aligning language models. Specifically, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to fine-tune GPT-3 to follow a broad class of written instructions (see Figure 2). This technique uses human preferences as a reward signal to fine-tune our models. We first hire a team of 40 contractors to label our data, based on their performance on a screening test (see Section 3.4 and Appendix B.1 for more details). We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to the OpenAI API3 and some labeler-written prompts, and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer. Finally, we use this RM as a reward function and fine-tune our supervised learning baseline to maximize this reward using the PPO algorithm (Schulman et al., 2017). We illustrate this process in Figure 2. This procedure aligns the behavior of GPT-3 to the stated preferences of a specific group of people (mostly our labelers and researchers), rather than any broader notion of “human values”; we discuss this further in Section 5.2. We call the resulting models InstructGPT.

我们专注于调整语言模型的微调方法。具体来说，我们使用来自人类反馈的强化学习（RLHF；Christiano 等人，2017 年；Stiennon 等人，2020 年）来微调 GPT-3 以遵循广泛的书面说明（见图 2）。该技术使用人类偏好作为奖励信号来微调我们的模型。我们首先聘请了一个由 40 名承包商组成的团队，根据他们在筛选测试中的表现来标记我们的数据（有关更多详细信息，请参见第 3.4 节和附录 B.1）。然后，我们在提交给 OpenAI API3 的提示（主要是英语）和一些标签编写的提示上收集所需输出行为的人工演示数据集，并使用它来训练我们的监督学习基线。接下来，我们收集了一个数据集，该数据集包含我们模型在更大的 API 提示集上的输出之间的人工标记比较。然后，我们在该数据集上训练一个奖励模型 (RM)，以预测我们的贴标机更喜欢哪个模型输出。最后，我们使用此 RM 作为奖励函数并微调我们的监督学习基线以使用 PPO 算法最大化此奖励（Schulman 等人，2017）。我们在图 2 中说明了这个过程。这个过程使 GPT-3 的行为与特定人群（主要是我们的贴标签者和研究人员）的既定偏好保持一致，而不是任何更广泛的“人类价值观”概念；我们将在 5.2 节中进一步讨论这个问题。我们将生成的模型称为 **InstructGPT**。

We mainly evaluate our models by having our labelers rate the quality of model outputs on our test set, consisting of prompts from held-out customers (who are not represented in the training data). We also conduct automatic evaluations on a range of public NLP datasets. We train three model sizes (1.3B, 6B, and 175B parameters), and all of our models use the GPT-3 architecture. Our main findings are as follows:

我们主要通过让我们的labelers对我们测试集上模型输出的质量进行评分来评估我们的模型，包括来自拒绝客户（未在训练数据中表示）的提示。我们还对一系列公共 NLP 数据集进行自动评估。我们训练三个模型size（1.3B、6B 和 175B 参数），我们所有的模型都使用 GPT-3 架构。我们的主要发现如下：

**Labelers significantly prefer InstructGPT outputs over outputs from GPT-3.** On our test set, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having over 100x fewer parameters. These models have the same architecture, and differ only by the fact that InstructGPT is fine-tuned on our human data. This result holds true even when we add a few-shot prompt to GPT-3 to make it better at following instructions. Outputs from our 175B InstructGPT are preferred to 175B GPT-3 outputs 85 ± 3% of the time, and preferred 71 ± 4% of the time to few-shot 175B GPT-3. InstructGPT models also generate more appropriate outputs according to our labelers, and more reliably follow explicit constraints in the instruction.

与 GPT-3 的输出相比，labelerrs明显更喜欢 InstructGPT 输出。在我们的测试集上，1.3B 参数 InstructGPT 模型的输出优于 175B GPT-3 的输出，尽管参数少了 100 多倍。这**些模型具有相同的架构，唯一不同的是 InstructGPT 是根据我们的人类数据进行微调的。即使我们向 GPT-3 添加少量提示以使其更好地遵循指令，这个结果仍然成立**。我们的 175B InstructGPT 的输出在 85±3% 的时间内优于 175B GPT-3 输出，在 71±4% 的时间内优于 few-shot 175B GPT-3。 InstructGPT 模型还根据我们的标签生成更合适的输出，并且更可靠地遵循指令中的明确约束。

**InstructGPT shows small improvements in toxicity over GPT-3, but not bias.** To measure toxicity, we use the RealToxicityPrompts dataset (Gehman et al., 2020) and conduct both automatic and human evaluations. InstructGPT models generate about 25% fewer toxic outputs than GPT-3 when prompted to be respectful. InstructGPT does not significantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.

InstructGPT 与 GPT-3 相比毒性略有改善，但在偏差上没有明显改善。为了测量毒性，我们使用 RealToxicityPrompts 数据集（Gehman 等人，2020 年）并进行自动和人工评估。当提示尊重时，InstructGPT 模型产生的毒性输出比 GPT-3 少约 25%。在 Winogender（Rudinger 等人，2018 年）和 CrowSPairs（Nangia 等人，2020 年）数据集上，InstructGPT 与 GPT-3 相比没有显着改善。

**We can minimize performance regressions on public NLP datasets by modifying our RLHF fine-tuning procedure.** During RLHF fine-tuning, we observe performance regressions compared to GPT-3 on certain public NLP datasets, notably SQuAD (Rajpurkar et al., 2018), DROP (Dua et al., 2019), HellaSwag (Zellers et al., 2019), and WMT 2015 French to English translation (Bojar et al., 2015). This is an example of an “alignment tax” since our alignment procedure comes at the cost of
lower performance on certain tasks that we may care about. We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of the pretraining distribution (PPO-ptx), without compromising labeler preference scores.

我们可以**通过修改我们的 RLHF 微调程序来最小化公共 NLP 数据集的性能回归**。在 RLHF 微调期间，我们在某些公共 NLP 数据集上观察到与 GPT-3 相比的性能回归，特别是 SQuAD（Rajpurkar 等人，2018 年）、DROP（Dua 等人，2019 年）、HellaSwag（Zellers 等人，2019 年） ), 和 WMT 2015 法英翻译 (Bojar et al., 2015)。这是一个“调整税”的例子，因为我们的调整程序是以降低我们可能关心的某些任务的性能为代价的。**通过将 PPO 更新与增加预训练分布 (PPO-ptx) 对数似然的更新混合**，我们可以大大减少这些数据集的性能回归，而不会影响标签偏好分数。

**Our models generalize to the preferences of “held-out” labelers that did not produce any training data.** To test the generalization of our models, we conduct a preliminary experiment with held-out labelers, and find that they prefer InstructGPT outputs to outputs from GPT-3 at about the same rate as our training labelers. However, more work is needed to study how these models perform on broader groups of users, and how they perform on inputs where humans disagree about the desired behavior.
我们的模型概括为不产生任何训练数据的“保留”标签的偏好。为了测试我们模型的泛化性，我们对留出的贴标器进行了初步实验，发现它们更喜欢 InstructGPT 输出而不是 GPT-3 的输出，其速率与我们的训练贴标器大致相同。然而，还需要做更多的工作来研究这些模型如何在更广泛的用户群体上执行，以及它们如何在人类不同意所需行为的输入上执行。