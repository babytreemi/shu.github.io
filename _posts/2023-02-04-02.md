---
title: 树酱的ai笔记(一)
data: 2023-02-04 13:00:00
categories: [学习笔记]
tags: [学习笔记]
math: true
---

# 1.人工智能概述

不能量化就不能优化

图像量化：RGB图，灰度图

文字量化：**one_hot**将文本进行向量化的方法具有的问题：忽略顺序，矩阵稀疏，歧义问题无法解决（词表固定，歧义较少通常还用onehot

文字的量化难度远高于图像

**场景举例：场景—向量-模型**

场景-向量：特征工程，传统机器学习对特征工程的要求很高，深度学习简化了特征工程的提取，只要调模型就ok

ai场景：离线和在线，离线上多用复杂模型，在线常用传统简单模型


>**性别表示：** 1.男[1] 女[0] 2.男[0,1] 女[1,0]
第二种方法会更好，因为第一种方法存在数量上的大小关系

>**年龄表示：**[0~100]
上面的表示是有缺陷的，比如25、26岁在电商的客户群体上是没有差异的，但是在数值上却有大小的差异，我们一般分段表示 eg.0-15 15-40 40-60 60-100 那么35岁可以表示为[0 1 0 0]
*综上所述性别和年龄可以用六维度向量表示*
也可以再加组合特征：比如性别2类，年龄4类，地区10类，消费能力5类，购物时间5类，共有26维特征，两两组和有更多特征

>**二手车平台汽车车辆颜色：**[0~255,0~255,0~255]R,G,B
大多数情况下这样表示不合适，因为车辆的颜色只有那么几种并没有大小之分，可行方法：总共二十种颜色，用一个二十维向量表示

>**今日头条推荐系统：**
一条新闻包括：video，img，text---具有的特征：文章长度，文章类别，关键词，发表时间，作者；美女图片，图像清晰度质量；视频播放的次数，播放的完成率
短向量拼接成长向量（强业务绑定）

# 2.线性回归(model1) 用来进行数值预测
 
```python   
# 模型训练和预测
# model是一个对象，他代表线性回归模型，他的成员变量已经有w和b，刚开始生成这两个值是随机的
model = LinearRegression()
# 调用函数开始不停的寻找w和b直到误差最小
model.fit(X_train,Y_train)  
model.coef_ # w
model.interceot_ # b
y_pred_train =model.predict(X.train)
```

**测试集和训练集：** 从训练集中用fit函数得到一个model然后再测试集中测试模型效果


```python
# 上面model训练完，接下来评估模型在训练集和测试集中的表现 sklearn求解训练集的mse（只有相对意义没有绝对意义，数量级不同会影响mse的大小）
train_mse = metrics.mean_squared_error(y_train,y_pred_train) # 训练集
y_pred_test = model.predict(X.test)
test_mse = metrics.mean_squared_error(y_test,y_pred_test)
```
**重要面试知识点**：

**测试集误差（mse2）一定大于训练集误差（mse1）吗？（不一定）**

测试集训练集都是从全量数据集中拿出的一部分，可以有交集也可以没有交集，测试集也可以刚好取到训练集那些刚好过直线的点，导致测试集的效果比训练集还好，但是在真实的环境中测试集的mse一般大于训练集的mse
测试集效果变好：1.训练数据集变大（搞机器学习一定要有大数据的支撑）2.数据尽量符合真实的环境（各个分布都来一点，多样性）

**泛化能力：** 能在测试集上表现良好的能力

为什么用 $(\hat{y}-y )^{2}$ 而不用 $\left | \hat{y} - y   \right | $
？

**直观理解：**

一堆数据点，有些难预测，有些容易预测，$(\hat{y}-y)^{2}$在将容易预测的点（即使得mse较小的点）的mse降低到一定程度之后，再在容易预测的点上进行优化的话，模型优化的收益就变小了，注意力就会转移到难预测的点上
如果用绝对值，那么$(\hat{y}-y )$得到提升收益不会变，注意力总在容易的点上，不容易的点接触不到，训练集变小了

**上面的收益指的是损失函数改变（降低）的值**


**数学理解：**

**绝对值在0点是没有导数的**（但是可以做一些近似）

**最小化平方误差本质上等于在误差服从高斯分布的情况下的最大似然估计**

我们通常认为误差$\varepsilon$服从标准正态分布，所以
给定一个$x_i$,模型输出$y_i$的概率为：

$$p(y_{i} |x_{i} )=\frac{1}{\sqrt{2\pi } } *exp(-\frac{\varepsilon _{i}^{2} }{2} )$$

假设n个样本点相互独立，所有x对应输出y的概率为所有的$p\left ( y_{i} \mid x_{i}  \right ) $的累乘：

$$L(y |x )=\prod_{i=1}^{n} \frac{1}{\sqrt{2\pi } } *exp(-\frac{\varepsilon _{i}^{2} }{2} )$$

取对数似然函数：

$$logL(y |x )=-\frac{n}{2}log2\pi-\frac{1}{2}\sum_{i=1}^{n}\varepsilon_{i}^{2}$$

去掉和$y_i$无关的项，最小化负对数似然：

$$neg\_logL(y |x )=\frac{1}{2}\sum_{i=1}^{n}\varepsilon_{i}^{2}=\frac{1}{2} \sum_{n}^{i=1}(\hat{y_i}-y_i )^{2}$$

上面的值越小，L越大，也就是说在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的

# 3.梯度下降

即求mse最小时的参数值，一般情况下为$\frac{\partial mse}{\partial \omega }=0$时$\omega$的取值：为什么不直接解方程呢？--计算量太大

$$mse = \frac{1}{n}\sum_{i=1}^{n}\left ( \omega x_{i}+b-y_{i} \right )^{2}$$
$$\frac{\partial mse}{\partial w} = \frac{1}{n} \sum_{i=1}^{n}2\left ( \omega x_{i}+b-y_{i} \right )*x_{i} $$
每次$\omega$的变化：
$$\omega \left ( t+1 \right ) = \omega \left ( t \right ) - \alpha \frac{\partial mse}{\partial w(t)}$$

$\alpha$:学习因子

**考虑存在多个最低点的抛物线情况**，有可能会停留在**局部极小**（线性回归里不存在这样的问题，后面会遇到，深度学习时很多）

**梯度下降法有什么问题呢？**
实操中n很大光是求偏导数的值极端两就已经很大，我们会随机抽取m个数据近似计算，不同的m求出来的值在真实值附近震荡，m越大震荡越小，实际操作中，m=16，64，128...

**为什么m不要选的太大**：运算量为$m$ 设m与真实值的震荡距离$d$,$d\propto  \frac{1}{\sqrt{m} }$,当$m$从100到10000时，计算量增加了100倍，但$d$只减小了10倍，实际操作中取小$m$多算几次


![fig1](https://github.com/babytreemi/markdownpicture/blob/main/1029371675505949_.pic.jpg?raw=true)
_取不同大小样本的梯度下降曲线示例_


>**训练集的mse越小越好吗？？no**
比如tb的算法下一条“异常数据”：月薪3000的男人买了一个10w的包包，mse很小就是学坏了（过拟合）我们不能无穷无尽训练；解决方法：从train中分出valid集(随机抽取)对$\alpha$和$t$进行预测
太大容易过拟合，适可而止

# 4.多元线性回归
$$y = \omega _{1}x_{1}+\omega _{2}x_{2}+...+\omega_{n}x_{n}+\omega_{0}=W^{T}X+W_{0}$$
```python
model = LinerRegtrssion()
model.fit(X_train,y_train)
print(model.coef) # w0
print(model.intercept_) # w向量
y_pred= model.predict(X_train)
print("mse:",metrics.mean_squard_error(y_train,y_pred))
```

使用线性回归的前提条件：数据尽量在一条直线上
如果图像特征为抛物线：可以通过对$X$进行变换的方式扩充变量：[2]-->[2,4]
```python
def extend_feature(x):
    return(x[0],x[0]*x[0])
```
变量拓展的越多效果就越好（而不会变差）

由于**taylor公式**任何函数服都可以分解为多项式的形式（不考虑计算成本的话）


## 线性回归的一些花式玩法
**增加随机特征：**$[x_{1}]$变为二维$[x_{1},x_{2}]$其中$x_{2}$随机产生，回归直线$y=w_{1}x_{1}+w_{2}x_{2}+w_{0}$
最终训练结果$w_{2}$趋向于0，体现了线性回归的**抗噪声能力**

**重复变量**: $[x_{1}]$变为二维$[x_{1},x_{1}]$,$[x_{2}]$变为二维$[x_{2},x_{2}]$回归直线结果不会发生变化，这是因为线性回归**抗信息冗余**，信息冗余时单独的$w_{i}$将不能代表权重


